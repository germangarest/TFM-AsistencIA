import os
import pickle
import streamlit as st
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, Document
from llama_index.embeddings.deepinfra import DeepInfraEmbeddingModel
from llama_index.llms.deepinfra import DeepInfraLLM
from pypdf import PdfReader

# Configuraci√≥n de la aplicaci√≥n Streamlit (Mover a la parte superior)
st.set_page_config(page_title="Asistente de Emergencia", page_icon="üö®", layout="centered")

# Configuraci√≥n de DeepInfra API
# Cargar variables de entorno desde .env
load_dotenv()
deepinfra_api_key = os.getenv("DEEPINFRA_TOKEN")

if not deepinfra_api_key:
    # Cargar la API Key de DeepInfra desde las secrets de Streamlit Cloud
    deepinfra_api_key = st.secrets["DEEPINFRA_TOKEN"]
    
    if not deepinfra_api_key:
        st.error("Falta la API Key de DeepInfra. Config√∫rala en un archivo .env o en el apartado secrets de stramlit cloud")
    

# Definir el modelo de embeddings y LLM de DeepInfra
# Configurar el modelo de embeddings con el nuevo modelo BGAI/bge-m3
Settings.embed_model = DeepInfraEmbeddingModel(
    model_id="BAAI/bge-m3",  # ID del modelo de embeddings de DeepInfra
    api_token=deepinfra_api_key,  # Token de la API
    normalize=True,  # Normalizaci√≥n opcional
    text_prefix="text: ",  # Prefijo de texto
    query_prefix="query: ",  # Prefijo de consulta
)

Settings.chunk_size = 1024

# Crear el modelo de embeddings
embedding_model = Settings.embed_model

# Configuraci√≥n centralizada del modelo LLM utilizando Settings
Settings.llm = DeepInfraLLM(
    model="meta-llama/Llama-3.3-70B-Instruct-Turbo",  # Modelo de DeepInfra
    api_key=deepinfra_api_key,  # Tu clave API de DeepInfra
    temperature=0,  # Configuraci√≥n de temperatura para control de creatividad
)

# Ruta del √≠ndice guardado
INDEX_PATH = "./vector_index.pkl"

@st.cache_resource
def extract_text_from_pdf(file_path):
    """Extrae el texto completo de un PDF usando pypdf."""
    reader = PdfReader(file_path)
    text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])
    return text

# Funci√≥n para cargar y guardar el √≠ndice y el modelo de embeddings
@st.cache_resource
def load_index():

    """Carga el √≠ndice de documentos. Si ya existe un √≠ndice guardado, lo reutiliza."""
    if os.path.exists(INDEX_PATH):
        # El "rb" es para la lectura binaria del documento
        with open(INDEX_PATH, "rb") as f:
            return pickle.load(f)
        
    # Los documentos est√°n en esta carpeta
    docs_folder = "docs"
    documents = []

    # Recorre cada archivo en la carpeta "docs"
    for file_name in os.listdir(docs_folder):
        file_path = os.path.join(docs_folder, file_name)
        if file_name.lower().endswith('.pdf'):
            # Extraer texto de PDF usando pypdf
            pdf_text = extract_text_from_pdf(file_path)
            documents.append(Document(text=pdf_text))
        else:
            # Para otros tipos de archivo se utiliza SimpleDirectoryReader
            documents.extend(SimpleDirectoryReader(docs_folder).load_data())

    print(f"Se indexaron {len(documents)} documentos.")

    # Usar DeepInfraEmbeddingModel para el √≠ndice
    index = VectorStoreIndex.from_documents(documents, embed_model=embedding_model)
    # Crear un motor de consulta en streaming con un top-k de similitud de 5
    query_engine = index.as_query_engine(streaming=True, similarity_top_k=5)

    # Guardamos el motor de consulta en un archivo con pickle
    # El "wb" es para el modo de escritura binaria
    with open(INDEX_PATH, "wb") as f:
        pickle.dump(query_engine, f)
    
    return query_engine

# Cargar el √≠ndice y el modelo de embeddings
query_engine = load_index()

# Configuraci√≥n de la conversaci√≥n
#prompt = "Eres un asistente experto en emergencias llamado AsistAI. Responde √∫nicamente preguntas relacionadas con la informaci√≥n en los documentos proporcionados. Si es necesario, complementa con informaci√≥n de internet, pero solo si est√° estrictamente relacionada con los documentos. Da respuestas claras, precisas y directas, sin explicaciones innecesarias. Intenta dar siempre la soluci√≥n al problema planteado con una respuesta concisa. Si te piden un n√∫mero posiblemente sea el de emergencia"

# opcion 2
#promt = "Eres un asistente experto en emergencias. Solo puedes responder preguntas relacionadas con la informaci√≥n contenida en los documentos proporcionados. Si la consulta no est√° dentro de estos temas, responde educadamente que no puedes ayudar.Si la respuesta requiere informaci√≥n adicional para ser m√°s precisa o actualizada, puedes buscar en internet, pero solo si est√° estrictamente relacionada con los temas cubiertos en los documentos. No generes respuestas con informaci√≥n no verificada o fuera de contexto.Responde de manera clara, precisa y √∫til, sin hacer referencia expl√≠cita a los documentos en tus respuestas."

prompt = """
Eres un asistente experto en emergencias llamado AsistAI. Responde √∫nicamente preguntas relacionadas con la informaci√≥n contenida en los documentos proporcionados. Si la pregunta no est√° cubierta por los documentos, indica que no puedes responder. Si es necesario, puedes complementar con informaci√≥n de internet, pero solo si est√° estrictamente relacionada con los documentos.
Cuando respondas, no hagas referencias al historial de la conversaci√≥n a menos que sea necesario para la respuesta actual. Conc√©dele mayor importancia a la consulta m√°s reciente. Si te piden un n√∫mero, es probable que sea un n√∫mero de emergencia.
Las respuestas deben ser claras, precisas, directas y breves. No incluyas explicaciones innecesarias.
"""

# Saludo
saludo = "üëã ¬°Hola! Soy **AsistAI**, tu asistente en situaciones de emergencia. Estoy aqu√≠ para ayudarte a resolver cualquier urgencia o aprender qu√© hacer en momentos cr√≠ticos. ¬øEn qu√© puedo ayudarte?"

# Inicializar historial de conversaci√≥n en la sesi√≥n de Streamlit
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "system", "content": prompt},
        {"role": "assistant", "content": saludo}
    ]

# Inicializar el estado de la sesi√≥n
if "processing" not in st.session_state:
    st.session_state.processing = False  # Variable de bloqueo para evitar m√∫ltiples consultas simult√°neas

# Configuraci√≥n de la aplicaci√≥n
st.title("üö® Asistente de Emergencia")
st.write("Pregunta sobre emergencias y obt√©n respuestas basadas en documentos.")

# Sidebar de la aplicaci√≥n
with st.sidebar:
    # Bot√≥n "Nuevo chat" para reiniciar la conversaci√≥n
    if st.button("Nuevo chat"):
        st.session_state.messages = [
            {"role": "system", "content": prompt},
            {"role": "assistant", "content": saludo}
        ]
        st.session_state.processing = False
    
    # Informaci√≥n sobre la aplicaci√≥n
    st.markdown("---")
    st.markdown("### Descripci√≥n")
    st.write("Este amable asistente de emergencia proporciona respuestas a preguntas basadas en documentos.")
    st.write("Hecho con ‚ù§Ô∏è por los estudiantes de **Accenture**")
    st.markdown("---")

# Mostrar historial de chat
for msg in st.session_state.messages:
    if msg["role"] == "user":
        st.chat_message("user", avatar="üßë‚Äçüíº").write(f"{msg['content']}")
    elif msg["role"] == "assistant":
        st.chat_message("assistant", avatar="ü§ñ").write(f"{msg['content']}")
    
# Entrada del usuario
if user_query := st.chat_input("Escribe tu pregunta...", disabled=st.session_state.processing):
    # Bloquear entrada de usuario durante la consulta
    st.session_state.processing = True
    
    # Agregar la pregunta al historial de mensajes
    st.session_state.messages.append({"role": "user", "content": user_query})
    
    # Mostrar la pregunta en el chat
    st.chat_message("user", avatar="üßë‚Äçüíº").write(f"{user_query}")
    
    # Recargar la interfaz para reflejar el cambio
    st.rerun()
    
    
# Procesar respuesta en una iteraci√≥n separada despu√©s de que la UI ya se haya actualizado
if st.session_state.processing and len(st.session_state.messages) > 1 and st.session_state.messages[-1]["role"] == "user":
        # Realizar consulta de respuesta en una iteraci√≥n separada
        with st.chat_message("assistant", avatar="ü§ñ"):
            # Crear un contexto con las preguntas y respuestas previas
            # Limitar historial a los √∫ltimos 5 mensajes
            conversation_history = "\n".join(
                [f"(msg['role'].capitalize()): {msg['content']}" for msg in st.session_state.messages]
            )
            
            # Colocamos el spinner en un contenedor vac√≠o para que se pueda manipular
            spinner_placeholder = st.empty()
            # Colocamos el contenedor de respuesta en un contenedor vac√≠o para que se pueda manipular
            response_placeholder = st.empty()
            # Variable para guardar la respuesta
            response_text = ""

            # Mostrar spinner junto al avatar
            with spinner_placeholder:
                with st.spinner("Pensando..."):
                # Generar respuesta en streaming
                    streaming_response = query_engine.query(f"Historial de conversaci√≥n:\n{conversation_history}\n\nNueva pregunta: {user_query}")
                    
                    # Quitar el spinner despu√©s de completar la operaci√≥n
                    spinner_placeholder.empty()
                    for fragment in streaming_response.response_gen:
                        response_text += fragment  # Acumular texto generado
                        response_placeholder.write(response_text)  # Mostrarlo 

            # Asignar el texto completo como respuesta final
            response = response_text

        # Agregar la respuesta al historial de mensajes
        st.session_state.messages.append({"role": "assistant", "content": response})
        
        # Desbloquear entrada de usuario despu√©s de obtener la consulta
        st.session_state.processing = False
        # Recargar la interfaz para reflejar el cambio
        st.rerun()
